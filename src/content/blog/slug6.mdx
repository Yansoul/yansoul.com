---
title: '不要只关注用户价值'
description: ''
date: '2025-06-18'
---

今天读到一篇很想当启发的文章，很有洞察的同时，又想当可落地、可执行。如果我未来几个月只能读一篇文章的话，我希望就是这一篇，足矣*（原文在结尾）*。

其实文章中的理念，在过去几个月产品设计的过程中，频繁地从脑子里闪现出来，譬如：

- 发现用户不只是在乎AI的回答质量，甚至更加在乎可信度；
- 任务执行不能全权交给AI，人需要对过程有感知、随时可掌控可接管；
- 在长任务生成前，需要通过预览功能提前构建起认知；

但这些想法，以及基于这些想法所产生的设计、编码，都是单点的、片面的、局部的，是看见哪里有问题就堵哪里，而自己从来没有从整体的角度，系统化结构化地去思考这个问题。

这篇文章则像是从一个抽象的角度，把我过去几个月在脑子里出现过的问题，从一个更高维度的层面，完全囊括了，所以在阅读的过程中，脑子里总会不断发现过去某个产品设计的细节，正好能在这篇文章中形成印证；又总能新意识到，很多自己先前从未发现的，但实际上一直存在的问题。

如果用一句大白话概括它给我最大的启发的话，那我想应该是：**并不是** **AI** **给用户创造价值的能力越强，用户就一定越愿意用这个 AI。**

### **CAIR (Confidence in** **AI** **Results) 用户对 AI 结果的信心：**

CAIR = 价值/（风险*纠正成本）

1. ##### 价值：用户在 AI 执行任务的结果中，取得的实际好处

这是常说的「为用户创造的价值」，包括时间节省、认知负荷减轻、工作质量提升；以及完成了一个原本无法完成的任务。

反过来，如果产品报错了，价值就是 0；如果该提取到的信息没提取出来，或者要等待 2 个小时，那价值也会大打折扣。

1. ##### 风险：AI 犯错时可能产生的负面后果

这不仅包含客观损失，也包含用户对损失的主观感知，同样的一个错误在不同场景下给用户带来的损失感受也是不同的。

一个例子是，如果我只是想了解一篇论文的大致思想，那么对AI总结的部分内容缺失是可以容忍的；但如果我的场景是精读它，甚至在我的文章中引用它的结论，那么 AI 的错误将会是一个学术严谨性性质的问题，我对这个产品的愤怒显然要大得多

1. ##### 纠错成本：用户发现 AI 错误后，将其恢复到满意状态所需付出的努力

包括识别错误的时间、理解错误原因的难度、实际修复的复杂性、重新验证是否修复的成本。这个成本，不仅包括时间，还包括对用户产生的认知负荷和情感成本

譬如综述生成出错的纠正成本，必然远远大于批量提取一个字段出错后的纠正成本。但如果批量提取出错后并不允许用户重试，那么意味着用户得重新上传，成本也是很大的。

这个 CAIR 公式很好地概括了用户对于白瓜学术的认可程度，用户对于这个产品是否有信心、是否愿意继续用下去甚至推荐给别人，不仅仅取决于它的 AI 是否可以产出很好的结果，还有 AI 出错对用户造成的代价、纠正错误的成本这两个因素——毕竟没有任何 AI 产品能做到 100% 不出错的。

### 基于 CAIR 的产品设计原则

有了上述的评价指标，那么整个产品设计的目标也变得更加清晰起来，即：提供更多价值、减少负面结果的概率、降低意外结果的纠错成本*（而不是只专注于提高价值这一个点）*

- **human in the loop:** 在关键决策点要求人类监督。引入人工 check 的代价是会增加用户的操作，收益是降低结果质量的风险。因此最佳的平衡是对结果影响最关键的几个节点，以最小的用户价值损失去撬动最大的 CAIR 效果。
  - 综述生成过程中，允许用户上传自己的文献，相当于把「内容来源」这个最关键最基本的决策点，完全交给人工。这确实是一个「我们现阶段没有能力解决，所以让用户做」的事情，但是能最大程度控制结果的稳定性；
  - 大纲是另一个很明显的，对最终效果影响很大的节点。向用户确认更多背景信息，也是能显著降低结果偏离预期的手段。
- **可逆性设计：**当 AI 的行为可以轻松撤销时，修复错误的心理成本和实际努力都会急剧下降，这可以将焦虑转变为信心
  - 当未来的综述生成过程可监控之后，需要允许用户随时打断并补充。就像我在 cursor 的 agent 模式中，可以随时终止 cursor 的编码行为并补充信息。
- **后果隔离：**提供预览模式、草稿等，为AI的产出结果创建一个安全空间，在用户用AI执行任务前，消除对意外后果的恐惧。当用户知道自己在和AI安全地实验，就算出错也无所谓时，对AI的探索和采纳都会显著提升
  - 上周上线的 showcase 其实某种程度上是在往这个方向上靠（只是当时自己只是凭着直接，并没有从更高的维度思考这个功能）。
- **透明性设计：**通过解释性的设计，告诉用户AI为什么要这么做，这种决定的原因是什么，可以降低用户心理层面的风险感受，也有助于用户对AI的能力建立更准确的期望
  - 未来的综述生成功能需要在前端透明化，也是这个理由。自己原先的思考只是想着要给用户「掌控感」，现在明白这种掌控感的根源，在于对用户风险预期的管理。
- **梯度设计：**先让用户体验低风险低价值的功能，逐步建立信心，然后逐步解锁更高价值但风险也更高的功能。渐进式的方式让用户不必一次面对产品中全部的复杂性
  - 其实自己曾注意到，使用过 Extract Data 的用户，在面对综述生成的付费弹窗时，付费意愿会更高。看到这「先通过低风险功能建立信心」的理念，才有恍然大悟之感。

这套 CAIR 给我提供了一套，在实践落地中，相当有意义的，定义产品的用户价值的指标，也提供了明确可评估的方向，相比于「模型能力」这种技术视角的评价，显然「用户对 AI 的信心」是一个更加产品视角的维度，影响它的因素也更加复杂。

我不知道这个「用户信心」标准是否依然只是片面的正确，但它已经足以把我在过去几个月遇到的问题给串联起来，从更高维度给出一个回答。

这篇文章比很多宏观分析、大而空的趋势洞察要有价值的多。值得反复去看。

**——最核心的问题不是你的 AI 结果足够好，而是你的用户是否有足够的信心和耐心，去探索发现它的价值**

> 附原文：https://mp.weixin.qq.com/s/gHRZFcuq7B_ytpQjN7AGdA?scene=1

---
